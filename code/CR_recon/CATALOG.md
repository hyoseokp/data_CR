# CR_recon ì½”ë“œ ì¹´íƒˆë¡œê·¸

**ëª©ì **: 128Ã—128 êµ¬ì¡° ì´ë¯¸ì§€ â†’ BGGR 2Ã—2 ìŠ¤í™íŠ¸ëŸ¼(30 bins) ì˜ˆì¸¡ ë”¥ëŸ¬ë‹ ì‹œìŠ¤í…œ

**ì£¼ìš” íŠ¹ì§•**:
- ëª¨ë“ˆí™”ëœ ì•„í‚¤í…ì²˜ (ëª¨ë¸/ì†ì‹¤/ë°ì´í„° ë¶„ë¦¬)
- ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ (WebSocket)
- 180ë„ íšŒì „ ë°ì´í„° ì¦ê°•
- ë°°ì¹˜ í¬ê¸°: 400, GPU: 12GB

---

## ğŸ“ í´ë” êµ¬ì¡°

```
CR_recon/
â”œâ”€â”€ configs/                    # ì„¤ì • íŒŒì¼ë“¤
â”‚   â”œâ”€â”€ default.yaml           # ê¸°ë³¸ ì„¤ì • (CNN_XAttn + MSE_Pearson, batch=400)
â”‚   â”œâ”€â”€ default_no_dashboard.yaml
â”‚   â”œâ”€â”€ default_weighted.yaml
â”‚   â””â”€â”€ test_cnn_gru.yaml
â”œâ”€â”€ data/                       # ë°ì´í„° ë¡œë”© & ì „ì²˜ë¦¬
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ dataset.py             # CRDataset: 180ë„ ì¦ê°• í¬í•¨
â”‚   â”œâ”€â”€ analyze_data.py        # ë°ì´í„° ë¶„ì„ ë„êµ¬
â”‚   â””â”€â”€ data_summary.md
â”œâ”€â”€ models/                     # ì‹ ê²½ë§ ëª¨ë¸ë“¤
â”‚   â”œâ”€â”€ __init__.py            # ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬
â”‚   â”œâ”€â”€ cnn_xattn.py           # CNN + Transformer Decoder (í˜„ì¬ ì‚¬ìš©)
â”‚   â””â”€â”€ cnn_gru.py             # CNN + GRU (ë¹„êµ ëª¨ë¸)
â”œâ”€â”€ losses/                     # ì†ì‹¤ í•¨ìˆ˜ë“¤
â”‚   â”œâ”€â”€ __init__.py            # ì†ì‹¤ í•¨ìˆ˜ ë ˆì§€ìŠ¤íŠ¸ë¦¬
â”‚   â”œâ”€â”€ mse_pearson.py         # MSE + Pearson correlation (í˜„ì¬ ì‚¬ìš©)
â”‚   â””â”€â”€ weighted_smooth.py     # MSE + smoothness ì •ê·œí™”
â”œâ”€â”€ dashboard/                  # ì‹¤ì‹œê°„ í•™ìŠµ ëŒ€ì‹œë³´ë“œ (FastAPI + WebSocket)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ server.py              # FastAPI ì„œë²„, WebSocket ê´€ë¦¬
â”‚   â”œâ”€â”€ hook.py                # Trainer callback
â”‚   â””â”€â”€ static/                # í”„ë¡ íŠ¸ì—”ë“œ (index.html, CSS, JS)
â”œâ”€â”€ train.py                    # CLI ì§„ì…ì 
â”œâ”€â”€ trainer.py                  # í•™ìŠµ ì—”ì§„ (ë©”ì¸ ë£¨í”„, ì²´í¬í¬ì¸íŠ¸)
â”œâ”€â”€ utils.py                    # ìœ í‹¸ë¦¬í‹° (config ë¡œë“œ ë“±)
â”œâ”€â”€ optimize_hyperparams.py     # í›ˆë ¨ ë¡œê·¸ ë¶„ì„ & ê¸°ë³¸ ì œì•ˆ
â”œâ”€â”€ SKILL_MODEL_OPTIMIZER.md    # ì§€ëŠ¥í˜• ëª¨ë¸ ìµœì í™” ìŠ¤í‚¬ ì •ì˜
â””â”€â”€ CATALOG.md                  # ì´ íŒŒì¼

```

---

## ğŸ“„ íŒŒì¼ ì„¤ëª…

### ğŸ¯ **ë©”ì¸ ì§„ì…ì **

#### `train.py` â­
```
ëª©ì : CLI ì§„ì…ì  + ìë™ ë°ì´í„° ì •ì œ
ì‚¬ìš©: python train.py --config configs/default.yaml [--resume checkpoint.pt]

ê¸°ëŠ¥:
  1. Config íŒŒì¼ íŒŒì‹±
  2. ì •ì œëœ ë°ì´í„° í™•ì¸:
     - ìˆìœ¼ë©´: dataset/bayer/*.npy ë¡œë“œ (ë§¤ìš° ë¹ ë¦„) âœ“
     - ì—†ìœ¼ë©´: preprocess_data.py ìë™ ì‹¤í–‰
  3. Trainer ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
  4. í›ˆë ¨ ì‹œì‘

ìë™ ì •ì œ ê¸°ëŠ¥:
  - í•¨ìˆ˜: ensure_preprocessed_data(cfg_dir)
  - ê²½ë¡œ: CR_recon/dataset/bayer/ (ìˆ˜ì •ë¨: data_CR-main â†’ dataset)
  - ì—­í• :
    * dataset/bayer/struct_*.npy, bayer_*.npy ì¡´ì¬ í™•ì¸
    * ì—†ìœ¼ë©´ subprocessë¡œ preprocess_data.py ì‹¤í–‰
    * íƒ€ì„ì•„ì›ƒ: 30ë¶„
    * ì‹¤íŒ¨ ì‹œ: ì˜¤ë¥˜ ë©”ì‹œì§€ ì¶œë ¥ í›„ ì¢…ë£Œ

ì›Œí¬í”Œë¡œìš°:
  python train.py --config configs/default.yaml
    â†“
  ì •ì œ ë°ì´í„° í™•ì¸ (dataset/bayer/)
    â”œâ”€ ìˆìŒ: mmap ë¡œë“œ â†’ ë°”ë¡œ í•™ìŠµ (ì´ˆ ë‹¨ìœ„)
    â””â”€ ì—†ìŒ: preprocess_data.py ì‹¤í–‰
      â”œâ”€ ì„±ê³µ: ìë™ ë¡œë“œ â†’ í•™ìŠµ
      â””â”€ ì‹¤íŒ¨: ì˜¤ë¥˜ ì¢…ë£Œ

í˜„í™©:
  âœ… ì •ì œëœ ë°ì´í„° íŒŒì¼ 6ê°œ ìƒì„±ë¨ (ì´ 1.7 GB)
  âœ… dataset/bayer/ ê²½ë¡œ í™•ì¸ë¨
  âœ… train.pyì—ì„œ ì •ìƒ ê°ì§€ ê°€ëŠ¥
```

#### `utils.py`
```
ëª©ì : ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
ì£¼ìš” ê¸°ëŠ¥:
  - load_config(): YAML ì„¤ì • íŒŒì¼ ë¡œë“œ
  - ê²½ë¡œ ì²˜ë¦¬
```

---

### ğŸ§  **í•µì‹¬: í•™ìŠµ ì—”ì§„**

#### `trainer.py` â­
```
ëª©ì : í›ˆë ¨ ë£¨í”„ ë° ìƒíƒœ ê´€ë¦¬
í´ë˜ìŠ¤: Trainer
ì£¼ìš” ë©”ì„œë“œ:
  - __init__(): ëª¨ë¸/loss/ë°ì´í„°ë¡œë” ì´ˆê¸°í™”
  - train(): ì „ì²´ í›ˆë ¨ ë£¨í”„ (epoch ë°˜ë³µ)
  - train_one_epoch(): í•œ epoch í›ˆë ¨
  - validate(): ê²€ì¦
  - save_checkpoint(): ì²´í¬í¬ì¸íŠ¸ ì €ì¥
  - load_checkpoint(): ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
  - log(): ë¡œê·¸ íŒŒì¼ì— ê¸°ë¡ (epochë§ˆë‹¤ flush)

íŠ¹ì§•:
  - ëŒ€ì‹œë³´ë“œ í†µí•© (WebSocket ì „ì†¡)
  - Callback ì‹œìŠ¤í…œ
  - AMP (Automatic Mixed Precision) ì§€ì›
  - Gradient clipping
  - Cosine annealing + warmup ìŠ¤ì¼€ì¤„ëŸ¬
  - ì‹¤ì‹œê°„ ë¡œê·¸ ì €ì¥ (ë²„í¼ë§ ìµœì†Œí™”)

ë¡œê·¸ í˜•ì‹:
  [EPOCH] N/total_epochs train_loss=X val_loss=Y best_val=Z lr=A
```

---

### ğŸ¨ **ëª¨ë¸ ì•„í‚¤í…ì²˜**

#### `models/__init__.py`
```
ëª©ì : ëª¨ë¸ íŒ©í† ë¦¬
í•¨ìˆ˜: get_model(name, **params) â†’ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
ë ˆì§€ìŠ¤íŠ¸ë¦¬:
  - "cnn_xattn": MetaSpec_CNNXAttn
  - "cnn_gru": MetaSpec_CNNGRU
```

#### `models/cnn_xattn.py` â­ (í˜„ì¬ ì‚¬ìš©)
```
ëª©ì : CNN backbone + Transformer decoder
ëª¨ë¸: MetaSpec_CNNXAttn
êµ¬ì¡°:
  1. Stem: 128Ã—128 â†’ 64Ã—64 (5Ã—5 conv)
  2. 4 Stages: CNN residual blocks (stride 2 for downsampling)
     - ê° stage: 2ê°œ residual blocks + circular padding
     - Channels: 64â†’96â†’128â†’192â†’256
  3. Global pooling: (256, 4, 4) â†’ 256D ë²¡í„°
  4. Transformer decoder:
     - Self-attention layers (8 heads, 4 layers)
     - ìŠ¤í™íŠ¸ëŸ¼ ì‹œí€€ìŠ¤ ìƒì„± (30 bins)
  5. Output: (B, 2, 2, 30) BGGR spectrum

íŠ¹ì§•:
  - Circular padding (ëŒ€ê°ì„  ëŒ€ì¹­ êµ¬ì¡° ë°˜ì˜)
  - GroupNorm + SiLU activation
  - Dropout ì •ê·œí™”
  - Positional encoding for Transformer

ì…ì¶œë ¥:
  ì…ë ¥: (B, 1, 128, 128)
  ì¶œë ¥: (B, 2, 2, 30)
```

#### `models/cnn_gru.py`
```
ëª©ì : CNN backbone + GRU (ê²½ëŸ‰ ë¹„êµ ëª¨ë¸)
êµ¬ì¡°:
  1. CNN backbone (CNN_XAttnê³¼ ë™ì¼)
  2. GRU ë ˆì´ì–´ (2 layers)
  3. Linear head (4 bins ì˜ˆì¸¡)

íŠ¹ì§•:
  - CNN_XAttnë³´ë‹¤ ë¹ ë¦„
  - ë©”ëª¨ë¦¬ íš¨ìœ¨
  - ì„±ëŠ¥ì€ CNN_XAttnì´ ë” ì¢‹ìŒ
```

---

### ğŸ’” **ì†ì‹¤ í•¨ìˆ˜**

#### `losses/__init__.py`
```
ëª©ì : ì†ì‹¤ í•¨ìˆ˜ íŒ©í† ë¦¬
í•¨ìˆ˜: get_loss(name, **params) â†’ loss_fn
ë ˆì§€ìŠ¤íŠ¸ë¦¬:
  - "mse_pearson": get_mse_pearson_loss
  - "weighted_smooth": get_weighted_smooth_loss
```

#### `losses/mse_pearson.py` â­ (í˜„ì¬ ì‚¬ìš©)
```
ëª©ì : MSE + Pearson ìƒê´€ê³„ìˆ˜
ê³µì‹: L = w_mse * MSE(pred, target) + w_corr * (1 - Pearson)
íŠ¹ì§•:
  - MSE: ì ˆëŒ€ê°’ ì˜¤ì°¨
  - Pearson: ìŠ¤í™íŠ¸ëŸ¼ í˜•íƒœ ìœ ì§€ (ìŠ¤ì¼€ì¼/ì‹œí”„íŠ¸ ë¬´ë³€)
  - í˜„ì¬ ê°€ì¤‘ì¹˜: w_mse=1.0, w_corr=0.2

ì¥ì :
  - ì‹¤ì¸¡ê°’ê³¼ì˜ ìˆ˜ì¹˜ì  ìœ ì‚¬ì„±
  - ìŠ¤í™íŠ¸ëŸ¼ í˜•íƒœ(íŒ¨í„´) ì¼ê´€ì„±
  - ìŠ¤ì¼€ì¼ ë³€í™”ì— ê°•ê±´
```

#### `losses/weighted_smooth.py`
```
ëª©ì : MSE + í‰í™œì„± ì •ê·œí™”
ê³µì‹: L = w_mse * MSE + w_smooth * smoothness_penalty
íŠ¹ì§•:
  - ì¸ì ‘ bin ê°„ ì°¨ì´ ìµœì†Œí™”
  - ë¬¼ë¦¬ì ìœ¼ë¡œ ë§¤ë„ëŸ¬ìš´ ìŠ¤í™íŠ¸ëŸ¼
```

---

### ğŸ“Š **ë°ì´í„° ë¡œë”© & ì „ì²˜ë¦¬**

#### `data/__init__.py`
```
ëª©ì : ë°ì´í„°ë¡œë” íŒ©í† ë¦¬
í•¨ìˆ˜: create_dataloaders(cfg) â†’ (train_loader, val_loader)
```

#### `data/dataset.py` â­
```
í´ë˜ìŠ¤: CRDataset
ëª©ì :
  1. ì •ì œëœ Bayer ë°ì´í„° ë˜ëŠ” ì›ë³¸ spectra ë¡œë“œ
  2. ì •ì œëœ ë°ì´í„° ì—†ìœ¼ë©´ ì¦‰ì‹œ ì •ì œ (í•„í„°ë§ + ë³€í™˜)
  3. 180ë„ íšŒì „ ë°ì´í„° ì¦ê°•
  4. ë°°ì¹˜ ì œê³µ

ë°ì´í„° ì²˜ë¦¬ (ìš°ì„ ìˆœìœ„):
  1. ì •ì œëœ ë°ì´í„° í™•ì¸:
     - bayer/struct_*.npy, bayer/bayer_*.npy ì¡´ì¬?
     - YES: ë°”ë¡œ ë¡œë“œ â†’ ë§¤ìš° ë¹ ë¦„ âœ“
     - NO: ì›ë³¸ì—ì„œ ì¦‰ì‹œ ì •ì œ â†“

  2. ì •ì œ (í•„ìš”ì‹œ):
     - struct: (N, 1, 128, 128) uint8 â†’ (M, 1, 128, 128) [0,255]
     - spectra: (N, 3, 301) float32 â†’ (M, 2, 2, 30) Bayer
       * í•„í„°ë§: 0-íŒ¨ë”© ìƒ˜í”Œ ì œì™¸
       * ë³€í™˜: RGB â†’ Bayer [[R,G],[G,B]]
       * ë‹¤ìš´ìƒ˜í”Œ: 301 bins â†’ 30 bins

  3. 180ë„ ì¦ê°•:
     - ì›ë³¸ Mê°œ + íšŒì „ Mê°œ = 2Mê°œ (augment=True)
     - íšŒì „: Râ†”B êµí™˜, struct flip

  4. __getitemì—ì„œ:
     - struct: [-1, 1] ì •ê·œí™” (map_to_pm1=True)
     - spectrum: ë¶€í˜¸ ë°˜ì „ (ìŒìˆ˜ â†’ ì–‘ìˆ˜)

ì¶œë ¥:
  - struct: (1, 128, 128) float32 âˆˆ [-1, 1]
  - spectrum: (2, 2, 30) float32 Bayer pattern
    * [0,0]=R, [0,1]=G, [1,0]=G, [1,1]=B

ë§¤ê°œë³€ìˆ˜:
  - augment_180: bool (ì¦ê°• í™œì„±í™”)
  - out_len: int (ì¶œë ¥ bins, ê¸°ë³¸ 30)
  - map_to_pm1: bool (ì •ê·œí™”, ê¸°ë³¸ True)

ì„±ëŠ¥:
  - ì •ì œ ë°ì´í„° ìˆìŒ: ì´ˆ ë‹¨ìœ„ ë¡œë“œ (mmap)
  - ì •ì œ ë°ì´í„° ì—†ìŒ: ì²« ì‹¤í–‰ë§Œ ~30ì´ˆ ì •ì œ
```

#### `preprocess_data.py` â­
```
ëª©ì : ì›ë³¸ ë°ì´í„°ë¥¼ Bayer íŒ¨í„´ìœ¼ë¡œ ì „ì²˜ë¦¬í•˜ì—¬ ì €ì¥
ì‚¬ìš©: python preprocess_data.py (ìë™ ì‹¤í–‰ or ìˆ˜ë™ ì‹¤í–‰)

ê¸°ëŠ¥:
  1. ì›ë³¸ íŒŒì¼ ë¡œë“œ
     - struct: (N, 1, 128, 128) from data_CR-main/
     - spectra: (N, 3, 301) from data_CR-main/

  2. ìœ íš¨ ìƒ˜í”Œ í•„í„°ë§
     - 0-íŒ¨ë”© ìƒ˜í”Œ ì œì™¸ (200,000 â†’ 105,601)

  3. Bayer íŒ¨í„´ ë³€í™˜
     - (3, 301) â†’ (2, 2, 30)
     - ë ˆì´ì•„ì›ƒ: [[R,G],[G,B]] (ì¤‘ìš”: Râ‰ B, G1=G2)
     - ë‹¤ìš´ìƒ˜í”Œ: 301 bins â†’ 30 bins

  4. 180ë„ íšŒì „ ìƒì„±
     - Râ†”B êµí™˜ (ëŒ€ê°ì„  ëŒ€ì¹­ êµ¬ì¡° ë°˜ì˜)

  5. ì €ì¥ (dataset/bayer/) âœ…
     - struct_0.npy (859 MB), struct_1.npy (791 MB)
     - bayer_0.npy (25 MB), bayer_1.npy (23 MB)
     - bayer_rotated_0.npy (25 MB), bayer_rotated_1.npy (23 MB)

ì„±ëŠ¥:
  - ì¶œë ¥: ì•½ 1.7 GB (struct 1.65 GB + bayer 0.05 GB)
  - ì‹œê°„: ì•½ 4-5ì´ˆ (ì „ì²´ ì •ì œ)
  - ë¡œë“œ: ~100ms (mmap)

í†µí•©:
  - train.py ì‹¤í–‰ ì‹œ ìë™ í˜¸ì¶œ (ensure_preprocessed_data)
  - ì •ì œëœ ë°ì´í„° ìˆìœ¼ë©´: ì¬ì‹¤í–‰ ìŠ¤í‚µ
  - ì •ì œëœ ë°ì´í„° ì—†ìœ¼ë©´: subprocess ì‹¤í–‰ (íƒ€ì„ì•„ì›ƒ 30ë¶„)
  - ìˆ˜ë™ ì‹¤í–‰ë„ ê°€ëŠ¥: python preprocess_data.py

ìµœì‹  ìƒíƒœ: âœ… ì™„ë£Œ
  - 6ê°œ íŒŒì¼ ì„±ê³µ ìƒì„±
  - train.py ê²½ë¡œ ë™ê¸°í™”ë¨
  - dataset.py í˜¸í™˜ë¨
```

#### `data/analyze_data.py`
```
ëª©ì : ë°ì´í„°ì…‹ ë¶„ì„ ë„êµ¬
ê¸°ëŠ¥:
  - ë°ì´í„° í¬ê¸°, ë²”ìœ„ í™•ì¸
  - ìƒ˜í”Œ ì‹œê°í™”
  - í†µê³„ ê³„ì‚°
```

---

### ğŸ“º **ëŒ€ì‹œë³´ë“œ (ì‹¤ì‹œê°„ ì‹œê°í™”)**

#### `dashboard/server.py` â­
```
ëª©ì : FastAPI + WebSocket ëŒ€ì‹œë³´ë“œ ì„œë²„
í´ë˜ìŠ¤: DashboardServer
í¬íŠ¸: 8501 (ê¸°ë³¸ê°’)

ì£¼ìš” ë©”ì„œë“œ:
  - start(): ì„œë²„ ì‹œì‘ (ë³„ë„ ìŠ¤ë ˆë“œ)
  - stop(): ì„œë²„ ì¢…ë£Œ
  - push_update(): epoch ê²°ê³¼ ì „ì†¡
  - push_progress(): batch ì§„í–‰ ìƒí™© ì „ì†¡
  - reset_state(): í›ˆë ¨ ì‹œì‘ ì‹œ ìƒíƒœ ì´ˆê¸°í™”

ìƒíƒœ (self.state):
  - epoch, total_epochs, lr
  - train_loss, val_loss, best_val
  - train_losses, val_losses (íˆìŠ¤í† ë¦¬)
  - progress (stage, batch, total_batches, current_loss)
  - sample (ì„ íƒì‚¬í•­)

WebSocket:
  - í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì‹œ í˜„ì¬ ìƒíƒœ ì „ì†¡
  - epoch/batch ì™„ë£Œ ì‹œ ì—…ë°ì´íŠ¸
```

#### `dashboard/hook.py`
```
ëª©ì : Trainer callback
í•¨ìˆ˜: DashboardHook(trainer)
ê¸°ëŠ¥:
  - epoch ì™„ë£Œ í›„ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
  - ëŒ€ì‹œë³´ë“œì— ì „ì†¡
```

#### `dashboard/static/index.html`
```
ëª©ì : í”„ë¡ íŠ¸ì—”ë“œ (ë¸Œë¼ìš°ì € ì‹œê°í™”)
ê¸°ëŠ¥:
  - ì†ì‹¤ ê·¸ë˜í”„ (Chart.js)
  - ì…ë ¥ ì´ë¯¸ì§€ ì‹œê°í™” (Canvas)
  - GT vs ì˜ˆì¸¡ ìŠ¤í™íŠ¸ëŸ¼ ë¹„êµ
  - Epoch/Batch ì§„í–‰ ë°”
  - ëª¨ë¸/ì†ì‹¤ ì •ë³´ (LaTeX with KaTeX)
  - LocalStorageë¡œ ìƒˆë¡œê³ ì¹¨ í›„ì—ë„ ë°ì´í„° ìœ ì§€
```

---

### ğŸ”§ **í›ˆë ¨ ë¡œê·¸ ë¶„ì„ & ìµœì í™”**

#### `optimize_hyperparams.py`
```
ëª©ì : í›ˆë ¨ ë¡œê·¸ ë¶„ì„ í›„ ê¸°ë³¸ ê°œì„  ì œì•ˆ
í•¨ìˆ˜:
  - parse_train_log(): ë¡œê·¸ íŒŒì‹±
  - analyze_performance(): ì§€í‘œ ê³„ì‚°
  - generate_suggestions(): ê·œì¹™ ê¸°ë°˜ ì œì•ˆ

ì œì•ˆ ë²”ìœ„:
  - Learning rate ì¡°ì •
  - ëª¨ë¸ ì „í™˜ (CNN_XAttn â†” CNN_GRU)
  - ì†ì‹¤ í•¨ìˆ˜ ì „í™˜
  - Weight decay, batch size ì¡°ì •

ì‚¬ìš©: python optimize_hyperparams.py --log outputs/train_log.txt
```

#### `SKILL_MODEL_OPTIMIZER.md` â­
```
ëª©ì : ì§€ëŠ¥í˜• ëª¨ë¸ ìµœì í™” ìŠ¤í‚¬ ì •ì˜
ì‚¬ìš©: ## ëª¨ë¸ optimizing í•´ì¤˜ (ë˜ëŠ” /model-optimizer)

í”„ë¡œì„¸ìŠ¤:
  1. ëª¨ë“  train_log.txt ìˆ˜ì§‘
  2. ì„±ëŠ¥ ì§€í‘œ ê³„ì‚° (ê°œì„ ìœ¨, ìˆ˜ë ´ì„±, ê³¼ì í•©ë„)
  3. íŒ¨í„´ ì¸ì‹ (ì–´ë–¤ ì„¤ì •ì´ íš¨ê³¼ì ì¸ê°€)
  4. ê·¼ë³¸ ì›ì¸ ë¶„ì„ (ì™œ ê°œì„  ì •ì²´ë¨?)
  5. ì°½ì˜ì  ì œì•ˆ (ìƒˆë¡œìš´ ì•„ì´ë””ì–´)
  6. ìµœê³  ì „ëµ ì¶”ì²œ
  7. ì‹¤í–‰ (ë™ì˜ ì‹œ)

ì œì•ˆ ë²”ìœ„:
  - ìƒˆë¡œìš´ ëª¨ë¸ ì•„í‚¤í…ì²˜
  - ìƒˆë¡œìš´ ì†ì‹¤ í•¨ìˆ˜
  - í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì •
  - ì¡°í•© ì „ëµ
```

---

### âš™ï¸ **ì„¤ì • íŒŒì¼**

#### `configs/default.yaml` â­ (í˜„ì¬ í™œìš©)
```yaml
# ë°ì´í„°
data:
  struct_files: [binary_dataset_128_0.npy, binary_dataset_128_1.npy]
  spectra_files: [spectra_latest_0.npy, spectra_latest_1.npy]
  out_len: 30
  batch_size: 400            # â† ìµœì í™”ë¨ (ì›ë˜ 64)
  augment_180: true          # â† 180ë„ íšŒì „ ì¦ê°•
  train_ratio: 0.95

# ëª¨ë¸
model:
  name: cnn_xattn
  params:
    out_len: 30
    d_model: 256
    nhead: 8
    dec_layers: 4
    cnn_dropout: 0.05
    tr_dropout: 0.1
    head_dropout: 0.2
    use_circular_padding: true

# ì†ì‹¤
loss:
  name: mse_pearson
  params:
    w_mse: 1.0
    w_corr: 0.2

# í›ˆë ¨
training:
  epochs: 300
  lr: 0.001
  weight_decay: 0.005
  grad_clip: 1.0
  use_amp: true             # Automatic Mixed Precision
  warmup_ratio: 0.05
  save_every: 10

# ëŒ€ì‹œë³´ë“œ
dashboard:
  enabled: true
  port: 8501

output:
  dir: outputs/
  log_file: train_log.txt
```

#### `configs/test_cnn_gru.yaml`
```
CNN_GRU ëª¨ë¸ë¡œ ë¹„êµ í…ŒìŠ¤íŠ¸ìš©
```

#### `configs/default_weighted.yaml`
```
Weighted_Smooth ì†ì‹¤í•¨ìˆ˜ í…ŒìŠ¤íŠ¸ìš©
```

#### `configs/default_no_dashboard.yaml`
```
ëŒ€ì‹œë³´ë“œ ì—†ì´ í›ˆë ¨ (ì†ë„ í…ŒìŠ¤íŠ¸)
```

---

## ğŸš€ **ì‚¬ìš© ë°©ë²•**

### 1. ê¸°ë³¸ í›ˆë ¨
```bash
cd CR_recon
python train.py --config configs/default.yaml
```

### 2. ì¤‘ë‹¨ëœ í›ˆë ¨ ì¬ê°œ
```bash
python train.py --config configs/default.yaml --resume outputs/cnn_xattn_best.pt
```

### 3. ë‹¤ë¥¸ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸
```bash
python train.py --config configs/test_cnn_gru.yaml
```

### 4. í›ˆë ¨ ë¡œê·¸ ë¶„ì„
```bash
python optimize_hyperparams.py --log outputs/train_log.txt
```

### 5. ì§€ëŠ¥í˜• ëª¨ë¸ ìµœì í™”
```
## ëª¨ë¸ optimizing í•´ì¤˜
```

### 6. ëŒ€ì‹œë³´ë“œ ì ‘ì†
```
http://localhost:8501
```

---

## ğŸ“ˆ **í˜„ì¬ ì„±ëŠ¥**

```
Configuration: CNN_XAttn + MSE_Pearson + batch=400
Data: 200,640 train (180ë„ ì¦ê°•) + 5,281 val
GPU: NVIDIA 12GB

Epoch 5:
  - best_val_loss: 0.0781
  - train_loss: 0.0838
  - 55% ê°œì„  (from epoch 1: 0.174)

Est. Time:
  - í•œ epoch: ~2.5ë¶„
  - 300 epochs: ~12.5ì‹œê°„
```

---

## ğŸ”„ **í™•ì¥ ê°€ëŠ¥ì„±**

### ì‰½ê²Œ ì¶”ê°€ ê°€ëŠ¥í•œ í•­ëª©
1. **ìƒˆ ëª¨ë¸**: `models/my_model.py` + `models/__init__.py`ì— ë“±ë¡
2. **ìƒˆ ì†ì‹¤**: `losses/my_loss.py` + `losses/__init__.py`ì— ë“±ë¡
3. **ìƒˆ ì„¤ì •**: `configs/test_xxx.yaml` ìƒì„±

### êµ¬í˜„ ì˜ˆ
```python
# 1. ìƒˆ ëª¨ë¸ êµ¬í˜„
class MetaSpec_MyModel(nn.Module):
    def __init__(self, out_len=30, **params):
        super().__init__()
        # ... êµ¬í˜„ ...

    def forward(self, x):
        # ì…ë ¥: (B, 1, 128, 128)
        # ì¶œë ¥: (B, 2, 2, out_len)
        return output

# 2. ë ˆì§€ìŠ¤íŠ¸ë¦¬ì— ë“±ë¡
# models/__init__.pyì— ì¶”ê°€:
from .my_model import MetaSpec_MyModel
_MODELS["my_model"] = MetaSpec_MyModel
```

---

## ğŸ“ **ì£¼ìš” ê°œì„  ì‚¬í•­**

| í•­ëª© | ìƒíƒœ |
|------|------|
| 180ë„ íšŒì „ ì¦ê°• | âœ… êµ¬í˜„ë¨ (ë¡œë“œ ì‹œì ) |
| ë°°ì¹˜ ì‚¬ì´ì¦ˆ ìµœì í™” | âœ… 400ìœ¼ë¡œ ì„¤ì • |
| ë¡œê·¸ ì¦‰ì‹œ ì €ì¥ | âœ… flush + fsync |
| ëŒ€ì‹œë³´ë“œ ìƒíƒœ ì´ˆê¸°í™” | âœ… í›ˆë ¨ ì‹œì‘ ì‹œ |
| Circular padding | âœ… ëŒ€ê°ì„  ëŒ€ì¹­ ë°˜ì˜ |
| ë°ì´í„° ì •ì œ | âœ… 0-íŒ¨ë”© ìƒ˜í”Œ ì œì™¸ |
| ëª¨ë¸ ìµœì í™” ìŠ¤í‚¬ | âœ… SKILL_MODEL_OPTIMIZER.md |

---

**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: 2026-02-06
**ìƒíƒœ**: ìš´ì˜ ì¤‘ (300 epochs í›ˆë ¨ ì§„í–‰)

